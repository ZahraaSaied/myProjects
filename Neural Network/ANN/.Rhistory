setwd("/media/zahraa/venus/@current work/Machine_Learning_A-Z_Hands-On_Python_and_R_In_Data_Science/my practice/NLP")
install.packages("tm")
setwd("/media/zahraa/venus/@current work/Machine_Learning_A-Z_Hands-On_Python_and_R_In_Data_Science/my practice/NLP")
install.packages("SnowballC")
original_dataset = read.delim("Restaurant_Reviews.tsv", quote = "", stringsAsFactors = FALSE)
View(original_dataset)
View(original_dataset)
library(tm)
library(SnowballC)
install.packages("tm")
install.packages("tm")
library(tm)
library(SnowballC)
library(tm)
library(SnowballC)
corpus = VCorpus(original_dataset$Review)
corpus = VCorpus(VectorSource(original_dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
as.character(corpus[[1]])
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
as.character(corpus[[1]])
corpus = tm_map(corpus, removeWhiteSpace)
corpus = tm_map(corpus, removeWords, stopwords)
corpus = tm_map(corpus, removeWords, stopwords())
as.character(corpus[[1]])
corpus = tm_map(corpus, stemDocument)
as.character(corpus[[1]])
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, stripWhitespace)
as.character(corpus[[1]])
corpus = VCorpus(VectorSource(original_dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
dtm = DocumentTermMatrix(corpus)
dtm
View(original_dataset)
View(dtm)
dtm = removeSparseTerms(dtm, 0.9999)
View(dtm)
View(dtm)
# Create the bag of words
dtm = DocumentTermMatrix(corpus)
View(dtm)
dtm = removeSparseTerms(dtm, 0.999)
View(dtm)
dtm = removeSparseTerms(dtm, 0.9995)
View(dtm)
dtm = removeSparseTerms(dtm, 0.9998)
View(dtm)
View(dtm)
dtm = removeSparseTerms(dtm, 0.9999)
View(dtm)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
View(dataset)
dataset$Liked = original_dataset$Liked
library(caTools)
install.packages("caTools")
library(caTools)
library(caTools)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
dataset$Liked = factor(dataset$Liked, levels = c(0, 1))
library(caTools)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
install.packages("randomForest")
library(randomForest)
# Build the classification model
library(randomForest)
classifier = randomForest(x = dataset[-692],
y = dataset$Liked,
ntree = 10)
y_pred = predict(classifier, test_set[-692])
cm = table(test_set[-692], y_pred)
cm = table(test_set[692], y_pred)
cm = table(test_set[, 692], y_pred)
cm
original_dataset = read.delim("Restaurant_Reviews.tsv", quote = "", stringsAsFactors = FALSE)
# Clean the data
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(original_dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dtm
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = original_dataset$Liked
dataset$Liked = factor(dataset$Liked, levels = c(0, 1))
library(caTools)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
library(randomForest)
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 10)
# Predict the test set results
y_pred = predict(classifier, newdata = test_set[-692])
cm = table(test_set[, 692], y_pred)
cm
19 + 35
(81+65)/200
library(randomForest)
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 20)
y_pred = predict(classifier, newdata = test_set[-692])
# Build the confusion matrix
cm = table(test_set[, 692], y_pred)
library(randomForest)
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 50)
y_pred = predict(classifier, newdata = test_set[-692])
# Build the confusion matrix
cm = table(test_set[, 692], y_pred)
library(randomForest)
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 50)
# Predict the test set results
y_pred = predict(classifier, newdata = test_set[-692])
# Build the confusion matrix
cm = table(test_set[, 692], y_pred)
library(randomForest)
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 50)
# Predict the test set results
y_pred = predict(classifier, newdata = test_set[-692])
# Build the confusion matrix
cm = table(test_set[, 692], y_pred)
# Predict the test set results
y_pred = predict(classifier, newdata = test_set[-692])
# Build the confusion matrix
cm = table(test_set[, 692], y_pred)
library(randomForest)
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 50)
y_pred = predict(classifier, newdata = test_set[-692])
# Build the confusion matrix
cm = table(test_set[, 692], y_pred)
(80+70)/200
install.packages("h2o")
install.packages("RCurl")
install.packages("RCurl")
install.packages("h2o")
setwd("/media/zahraa/venus/@current work/Machine_Learning_A-Z_Hands-On_Python_and_R_In_Data_Science/my practice/ANN")
dataset = read.csv("Churn_Modelling.csv")
View(dataset)
dataset = dataset[3:14]
View(dataset)
dataset = dataset[4:14]
dataset = read.csv("Churn_Modelling.csv")
dataset = dataset[4:14]
View(dataset)
# Handle categorial variables as factors
dataset$Geography = as.numeric(factor(dataset$Geography,
levels = c('France', 'Spain', 'Germany'),
labels = c(1, 2, 3)))
dataset$Gender = as.numeric(factor(dataset$Gender,
levels = c('Male', 'Female'),
labels = c('0', '1')))
dataset = read.csv("Churn_Modelling.csv")
dataset = dataset[4:14]
dataset$Geography = as.numeric(factor(dataset$Geography,
levels = c('France', 'Spain', 'Germany'),
labels = c(1, 2, 3)))
dataset$Gender = as.numeric(factor(dataset$Gender,
levels = c('Male', 'Female'),
labels = c(0, 1)))
dataset$Gender = as.numeric(factor(dataset$Gender,
levels = c('Male', 'Female'),
labels = c(1, 2)))
# Import the dataset
dataset = read.csv("Churn_Modelling.csv")
dataset = dataset[4:14]
dataset$Geography = as.numeric(factor(dataset$Geography,
levels = c('France', 'Spain', 'Germany'),
labels = c(1, 2, 3)))
dataset$Gender = as.numeric(factor(dataset$Gender,
levels = c('Male', 'Female'),
labels = c(1, 2)))
library(caTools)
split = sample.split(dataset$Exited, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
View(test_set)
View(training_set)
# Feature scaling
training_set[-11] = scale(training_set[-11])
test_set[-11] = scale(test_set[-11])
library(h2o)
h2o.init(nthreads = -1)
View(dataset)
#build and fit the ANN
classifier = h2o.deeplearning(y = 'Exited',
training_frame = as.h2o(training_set),
activation = 'Rectifier',
hidden = c(6, 6),
epochs = 100,
train_samples_per_iteration = -2)
h2o.init(nthreads = -1)
classifier = h2o.deeplearning(y = 'Exited',
training_frame = as.h2o(training_set),
activation = 'Rectifier',
hidden = c(6, 6),
epochs = 100,
train_samples_per_iteration = -2)
y_pred = h2o.predict(classifier, newdata = as.h2o(test_set[-11]))
y_pred = (y_pred > 0.5)
y_pred = as.vector(y_pred)
cm = table(test_set[, 11] ,y_pred)
cm
(1529+179)/2000
